{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgDdf8f35bKubQ94uct9JN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debayan2004/BR-Tumor-Segmentation/blob/main/compute_parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6MMMwz35YBYh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from scipy.spatial.distance import directed_hausdorff"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_weights(y_true, num_classes=11, irrelevant_classes=[0, 10, 11], max_weight=50):\n",
        "    \"\"\"\n",
        "    Calculate class weights based on class proportions in the training dataset.\n",
        "    Classes with fewer voxels (rare classes) will have higher weights.\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth labels (one-hot encoded, shape: [batch_size, depth, height, width, num_classes]).\n",
        "        num_classes: Total number of classes in the dataset.\n",
        "        irrelevant_classes: List of labels to be excluded from weight calculation.\n",
        "        max_weight: Maximum allowable weight for any class to prevent excessive weighting.\n",
        "\n",
        "    Returns:\n",
        "        class_weights: Tensor of class weights, where the index corresponds to the class.\n",
        "    \"\"\"\n",
        "    # Sum over spatial dimensions (depth, height, width) to get the count of voxels for each class\n",
        "    num_voxels_per_class = tf.reduce_sum(y_true, axis=(0, 1, 2, 3))  # Shape: [num_classes]\n",
        "\n",
        "    # Calculate the total number of voxels across all classes\n",
        "    total_voxels = tf.reduce_sum(num_voxels_per_class)\n",
        "\n",
        "    # Inverse proportionality to calculate class weights\n",
        "    class_weights = total_voxels / (num_voxels_per_class + 1e-6)  # Avoid division by zero\n",
        "\n",
        "    # Scale weights logarithmically to avoid overly large weights\n",
        "    class_weights = tf.math.log1p(class_weights)  # log1p(x) = log(x + 1)\n",
        "\n",
        "    # Cap weights to avoid extreme values\n",
        "    class_weights = tf.minimum(class_weights, max_weight)\n",
        "\n",
        "    # Set weights for irrelevant classes to zero\n",
        "    irrelevant_mask = tf.reduce_sum(tf.one_hot(irrelevant_classes, depth=num_classes), axis=0)\n",
        "    class_weights = class_weights * (1 - irrelevant_mask)  # Zero out weights for irrelevant labels\n",
        "\n",
        "    return class_weights"
      ],
      "metadata": {
        "id": "WMPu6BRBZ3G9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_categorical_crossentropy_with_weights(y_true, y_pred, class_weights):\n",
        "    \"\"\"\n",
        "    Calculates normalized categorical cross-entropy loss with class weights for multi-class segmentation.\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth labels (one-hot encoded, shape: [batch_size, height, width, num_classes]).\n",
        "        y_pred: Predicted probabilities (after softmax, shape: [batch_size, height, width, num_classes]).\n",
        "        class_weights: A tensor or list of class weights (length should match number of classes).\n",
        "\n",
        "    Returns:\n",
        "        The normalized categorical cross-entropy loss.\n",
        "    \"\"\"\n",
        "    # Convert class_weights to a tensor if not already\n",
        "    class_weights = tf.convert_to_tensor(class_weights, dtype=tf.float32)\n",
        "\n",
        "    # Flatten y_true and y_pred, keeping the class dimension intact\n",
        "    y_true_flat = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])  # Shape: (total_voxels, num_classes)\n",
        "    y_pred_flat = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])  # Shape: (total_voxels, num_classes)\n",
        "\n",
        "    # Categorical cross-entropy loss (not averaged yet)\n",
        "    ce_loss = tf.keras.losses.categorical_crossentropy(y_true_flat, y_pred_flat)\n",
        "\n",
        "    # Get indices of the true classes from one-hot encoding\n",
        "    true_class_indices = tf.argmax(y_true_flat, axis=-1)\n",
        "\n",
        "    # Apply class weights\n",
        "    weighted_ce_loss = ce_loss * tf.gather(class_weights, true_class_indices)\n",
        "\n",
        "    # Normalize by taking the mean across all voxels\n",
        "    normalized_loss = tf.reduce_mean(weighted_ce_loss)\n",
        "\n",
        "    return normalized_loss"
      ],
      "metadata": {
        "id": "py7ez7MoZ5i8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_function(y_true, y_pred, class_weights):\n",
        "    \"\"\"\n",
        "    Custom loss function with class weights for imbalanced datasets.\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth labels (one-hot encoded, shape: [batch_size, depth, height, width, num_classes]).\n",
        "        y_pred: Predicted probabilities (softmax output, shape: [batch_size, depth, height, width, num_classes]).\n",
        "        class_weights: Class weights tensor calculated based on the dataset.\n",
        "\n",
        "    Returns:\n",
        "        Loss value for the batch.\n",
        "    \"\"\"\n",
        "    # Compute the weighted normalized categorical cross-entropy\n",
        "    loss = normalized_categorical_crossentropy_with_weights(y_true, y_pred, class_weights)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "UMCCpi6u8gb7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_function(y_true_sample, num_classes=11, irrelevant_classes=[0, 10, 11]):\n",
        "    \"\"\"\n",
        "    Prepare the custom loss function by excluding irrelevant classes.\n",
        "\n",
        "    Args:\n",
        "        y_true_sample: A sample of the ground truth labels to calculate weights.\n",
        "        num_classes: Total number of classes in the dataset.\n",
        "        irrelevant_classes: Classes to be excluded from the weighting calculation.\n",
        "\n",
        "    Returns:\n",
        "        A custom loss function ready for use.\n",
        "    \"\"\"\n",
        "    # Mask irrelevant classes by setting them to 0\n",
        "    mask = tf.reduce_sum(tf.one_hot(irrelevant_classes, depth=num_classes), axis=0)\n",
        "    y_true_sample = y_true_sample * (1 - mask)\n",
        "\n",
        "    # Calculate class weights excluding irrelevant classes\n",
        "    class_weights = calculate_class_weights(y_true_sample, num_classes=num_classes, irrelevant_classes=irrelevant_classes)\n",
        "\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        # Apply the custom loss function with calculated weights\n",
        "        return custom_loss_function(y_true, y_pred, class_weights)\n",
        "\n",
        "    return loss_fn"
      ],
      "metadata": {
        "id": "E5crDo748lDz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dice Coefficient Function\n",
        "def dice_coefficient(y_true, y_pred, threshold=0.5):\n",
        "    \"\"\"Calculate the Dice coefficient for multi-class segmentation.\"\"\"\n",
        "    smooth = 1e-6  # Avoid division by zero\n",
        "    y_true = tf.round(y_true)  # Convert to binary values\n",
        "    y_pred = tf.round(y_pred)  # Convert to binary values\n",
        "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])  # Sum intersection\n",
        "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])  # Sum of union\n",
        "    dice = 2 * intersection / (union + smooth)  # Calculate Dice coefficient\n",
        "    return tf.reduce_mean(dice)  # Mean over the batch\n"
      ],
      "metadata": {
        "id": "lJVxL05FF8ji"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Volume Similarity Function\n",
        "def volume_similarity(y_true, y_pred):\n",
        "    \"\"\"Calculate the volume similarity for multi-class segmentation.\"\"\"\n",
        "    y_true = tf.round(y_true)  # Convert to binary values\n",
        "    y_pred = tf.round(y_pred)  # Convert to binary values\n",
        "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])  # Intersection\n",
        "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])  # Union\n",
        "    volume_sim = intersection / (union + 1e-6)  # Volume similarity\n",
        "    return tf.reduce_mean(volume_sim)  # Mean over the batch\n"
      ],
      "metadata": {
        "id": "cW-SQYWNGmwn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hausdorff Distance Calculation Function\n",
        "def HausdorffDist(A, B):\n",
        "    \"\"\"Compute the Hausdorff distance between two point clouds.\"\"\"\n",
        "    # Find pairwise distance\n",
        "    D_mat = np.sqrt(inner1d(A, A)[np.newaxis].T + inner1d(B, B) - 2 * (np.dot(A, B.T)))\n",
        "    # Find DH\n",
        "    dH = np.max(np.array([np.max(np.min(D_mat, axis=0)), np.max(np.min(D_mat, axis=1))]))\n",
        "    return dH"
      ],
      "metadata": {
        "id": "DiSt9q5xGnLC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified Hausdorff Distance Calculation Function\n",
        "def ModHausdorffDist(A, B):\n",
        "    \"\"\"Compute the Modified Hausdorff Distance.\"\"\"\n",
        "    # Find pairwise distance\n",
        "    D_mat = np.sqrt(inner1d(A, A)[np.newaxis].T + inner1d(B, B) - 2 * (np.dot(A, B.T)))\n",
        "    # Calculating the forward HD: mean(min(each col))\n",
        "    FHD = np.mean(np.min(D_mat, axis=1))\n",
        "    # Calculating the reverse HD: mean(min(each row))\n",
        "    RHD = np.mean(np.min(D_mat, axis=0))\n",
        "    # Calculating MHD\n",
        "    MHD = np.max(np.array([FHD, RHD]))\n",
        "    return MHD, FHD, RHD\n",
        "\n",
        "# Hausdorff Distance Wrapper for TensorFlow\n",
        "def hausdorff_distance(y_true, y_pred):\n",
        "    \"\"\"Calculate the Hausdorff distance between the true and predicted binary masks.\"\"\"\n",
        "    # Convert to binary mask (0 or 1)\n",
        "    y_true = tf.round(y_true)\n",
        "    y_pred = tf.round(y_pred)\n",
        "\n",
        "    # Convert tensors to numpy arrays outside TensorFlow operations\n",
        "    y_true = y_true.numpy() if tf.executing_eagerly() else y_true\n",
        "    y_pred = y_pred.numpy() if tf.executing_eagerly() else y_pred\n",
        "\n",
        "    # Extract coordinates of non-zero voxels (the contour of the mask)\n",
        "    true_points = np.array(np.where(y_true > 0)).T\n",
        "    pred_points = np.array(np.where(y_pred > 0)).T\n",
        "\n",
        "    # Compute directed Hausdorff distance\n",
        "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
        "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
        "\n",
        "    # The Hausdorff distance is the maximum of these two directed distances\n",
        "    return np.max([forward_hausdorff, reverse_hausdorff])"
      ],
      "metadata": {
        "id": "2s9Jz0KsG9rn"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}