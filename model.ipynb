{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvcWYuFgy7Y8EdvmFRqMGs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debayan2004/BR-Tumor-Segmentation/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wBEqdPfURP84"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.layers import Add, Multiply\n",
        "\n",
        "def residual_block(x, filters, kernel_size=(3, 3, 3), l2_reg=1e-4):\n",
        "    \"\"\"\n",
        "    A residual block with two convolutional layers and a skip connection.\n",
        "    \"\"\"\n",
        "    shortcut = x\n",
        "    x = layers.Conv3D(filters, kernel_size, activation='relu', padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv3D(filters, kernel_size, activation=None, padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = Add()([x, shortcut])  # Skip connection\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def attention_gate(x, g, inter_channel):\n",
        "    \"\"\"\n",
        "    An attention gate for the skip connection.\n",
        "    x: Input feature map from the encoder\n",
        "    g: Gating signal from the decoder\n",
        "    inter_channel: Number of intermediate channels\n",
        "    \"\"\"\n",
        "    theta_x = layers.Conv3D(inter_channel, kernel_size=1, strides=1, padding='same')(x)\n",
        "    phi_g = layers.Conv3D(inter_channel, kernel_size=1, strides=1, padding='same')(g)\n",
        "    attention = layers.Add()([theta_x, phi_g])\n",
        "    attention = layers.Activation('relu')(attention)\n",
        "    attention = layers.Conv3D(1, kernel_size=1, strides=1, padding='same')(attention)\n",
        "    attention = layers.Activation('sigmoid')(attention)\n",
        "    x = Multiply()([x, attention])\n",
        "    return x\n",
        "\n",
        "def build_3d_unet_with_residual_and_attention(input_shape=(8, 16, 16, 3), num_classes=32, dropout_rate=0.3, l2_reg=1e-4):\n",
        "    \"\"\"\n",
        "    Build a 3D U-Net with residual blocks and attention mechanisms.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder Level 1: 64 filters\n",
        "    x1 = residual_block(inputs, 64, l2_reg=l2_reg)\n",
        "    p1 = layers.MaxPooling3D(pool_size=(2, 2, 2))(x1)\n",
        "\n",
        "    # Encoder Level 2: 128 filters\n",
        "    x2 = residual_block(p1, 128, l2_reg=l2_reg)\n",
        "    p2 = layers.MaxPooling3D(pool_size=(2, 2, 2))(x2)\n",
        "\n",
        "    # Encoder Level 3: 256 filters\n",
        "    x3 = residual_block(p2, 256, l2_reg=l2_reg)\n",
        "    p3 = layers.MaxPooling3D(pool_size=(2, 2, 2))(x3)\n",
        "\n",
        "    # Encoder Level 4: 512 filters\n",
        "    x4 = residual_block(p3, 512, l2_reg=l2_reg)\n",
        "    p4 = layers.MaxPooling3D(pool_size=(2, 2, 2))(x4)\n",
        "\n",
        "    # Bottleneck\n",
        "    x = residual_block(p4, 1024, l2_reg=l2_reg)\n",
        "\n",
        "    # Decoder Level 4 (up to Level 3)\n",
        "    x = layers.Conv3DTranspose(512, kernel_size=(2, 2, 2), strides=(2, 2, 2), padding='same')(x)\n",
        "    x4 = attention_gate(x4, x, inter_channel=256)  # Attention gate\n",
        "    x = layers.concatenate([x, x4])\n",
        "    x = residual_block(x, 512, l2_reg=l2_reg)\n",
        "\n",
        "    # Decoder Level 3 (up to Level 2)\n",
        "    x = layers.Conv3DTranspose(256, kernel_size=(2, 2, 2), strides=(2, 2, 2), padding='same')(x)\n",
        "    x3 = attention_gate(x3, x, inter_channel=128)  # Attention gate\n",
        "    x = layers.concatenate([x, x3])\n",
        "    x = residual_block(x, 256, l2_reg=l2_reg)\n",
        "\n",
        "    # Decoder Level 2 (up to Level 1)\n",
        "    x = layers.Conv3DTranspose(128, kernel_size=(2, 2, 2), strides=(2, 2, 2), padding='same')(x)\n",
        "    x2 = attention_gate(x2, x, inter_channel=64)  # Attention gate\n",
        "    x = layers.concatenate([x, x2])\n",
        "    x = residual_block(x, 128, l2_reg=l2_reg)\n",
        "\n",
        "    # Decoder Level 1 (up to original resolution)\n",
        "    x = layers.Conv3DTranspose(64, kernel_size=(2, 2, 2), strides=(2, 2, 2), padding='same')(x)\n",
        "    x1 = attention_gate(x1, x, inter_channel=32)  # Attention gate\n",
        "    x = layers.concatenate([x, x1])\n",
        "    x = residual_block(x, 64, l2_reg=l2_reg)\n",
        "\n",
        "    # Output Layer\n",
        "    outputs = layers.Conv3D(num_classes, kernel_size=(1, 1, 1), activation='softmax', padding='same')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = models.Model(inputs, outputs)\n",
        "\n",
        "    return model\n"
      ]
    }
  ]
}