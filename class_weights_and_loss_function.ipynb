{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1WoDENhoyuObXwl4n/Hjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debayan2004/BR-Tumor-Segmentation/blob/main/class_weights_and_loss_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6MMMwz35YBYh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_weights(y_true, num_classes=11, irrelevant_classes=[0, 10, 11], max_weight=50):\n",
        "    \"\"\"\n",
        "    Calculate class weights based on class proportions in the training dataset.\n",
        "    Classes with fewer voxels (rare classes) will have higher weights.\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth labels (one-hot encoded, shape: [batch_size, depth, height, width, num_classes]).\n",
        "        num_classes: Total number of classes in the dataset.\n",
        "        irrelevant_classes: List of labels to be excluded from weight calculation.\n",
        "        max_weight: Maximum allowable weight for any class to prevent excessive weighting.\n",
        "\n",
        "    Returns:\n",
        "        class_weights: Tensor of class weights, where the index corresponds to the class.\n",
        "    \"\"\"\n",
        "    # Sum over spatial dimensions (depth, height, width) to get the count of voxels for each class\n",
        "    num_voxels_per_class = tf.reduce_sum(y_true, axis=(0, 1, 2, 3))  # Shape: [num_classes]\n",
        "\n",
        "    # Calculate the total number of voxels across all classes\n",
        "    total_voxels = tf.reduce_sum(num_voxels_per_class)\n",
        "\n",
        "    # Inverse proportionality to calculate class weights\n",
        "    class_weights = total_voxels / (num_voxels_per_class + 1e-6)  # Avoid division by zero\n",
        "\n",
        "    # Scale weights logarithmically to avoid overly large weights\n",
        "    class_weights = tf.math.log1p(class_weights)  # log1p(x) = log(x + 1)\n",
        "\n",
        "    # Cap weights to avoid extreme values\n",
        "    class_weights = tf.minimum(class_weights, max_weight)\n",
        "\n",
        "    # Set weights for irrelevant classes to zero\n",
        "    irrelevant_mask = tf.reduce_sum(tf.one_hot(irrelevant_classes, depth=num_classes), axis=0)\n",
        "    class_weights = class_weights * (1 - irrelevant_mask)  # Zero out weights for irrelevant labels\n",
        "\n",
        "    return class_weights"
      ],
      "metadata": {
        "id": "WMPu6BRBZ3G9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_categorical_crossentropy_with_weights(y_true, y_pred, class_weights):\n",
        "    \"\"\"\n",
        "    Calculates normalized categorical cross-entropy loss with class weights for multi-class segmentation.\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth labels (one-hot encoded, shape: [batch_size, height, width, num_classes]).\n",
        "        y_pred: Predicted probabilities (after softmax, shape: [batch_size, height, width, num_classes]).\n",
        "        class_weights: A tensor or list of class weights (length should match number of classes).\n",
        "\n",
        "    Returns:\n",
        "        The normalized categorical cross-entropy loss.\n",
        "    \"\"\"\n",
        "    # Convert class_weights to a tensor if not already\n",
        "    class_weights = tf.convert_to_tensor(class_weights, dtype=tf.float32)\n",
        "\n",
        "    # Flatten y_true and y_pred, keeping the class dimension intact\n",
        "    y_true_flat = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])  # Shape: (total_voxels, num_classes)\n",
        "    y_pred_flat = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])  # Shape: (total_voxels, num_classes)\n",
        "\n",
        "    # Categorical cross-entropy loss (not averaged yet)\n",
        "    ce_loss = tf.keras.losses.categorical_crossentropy(y_true_flat, y_pred_flat)\n",
        "\n",
        "    # Get indices of the true classes from one-hot encoding\n",
        "    true_class_indices = tf.argmax(y_true_flat, axis=-1)\n",
        "\n",
        "    # Apply class weights\n",
        "    weighted_ce_loss = ce_loss * tf.gather(class_weights, true_class_indices)\n",
        "\n",
        "    # Normalize by taking the mean across all voxels\n",
        "    normalized_loss = tf.reduce_mean(weighted_ce_loss)\n",
        "\n",
        "    return normalized_loss"
      ],
      "metadata": {
        "id": "py7ez7MoZ5i8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_function(y_true, y_pred, class_weights):\n",
        "    \"\"\"\n",
        "    Custom loss function with class weights for imbalanced datasets.\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth labels (one-hot encoded, shape: [batch_size, depth, height, width, num_classes]).\n",
        "        y_pred: Predicted probabilities (softmax output, shape: [batch_size, depth, height, width, num_classes]).\n",
        "        class_weights: Class weights tensor calculated based on the dataset.\n",
        "\n",
        "    Returns:\n",
        "        Loss value for the batch.\n",
        "    \"\"\"\n",
        "    # Compute the weighted normalized categorical cross-entropy\n",
        "    loss = normalized_categorical_crossentropy_with_weights(y_true, y_pred, class_weights)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "UMCCpi6u8gb7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_function(y_true_sample, num_classes=11, irrelevant_classes=[0, 10, 11]):\n",
        "    \"\"\"\n",
        "    Prepare the custom loss function by excluding irrelevant classes.\n",
        "\n",
        "    Args:\n",
        "        y_true_sample: A sample of the ground truth labels to calculate weights.\n",
        "        num_classes: Total number of classes in the dataset.\n",
        "        irrelevant_classes: Classes to be excluded from the weighting calculation.\n",
        "\n",
        "    Returns:\n",
        "        A custom loss function ready for use.\n",
        "    \"\"\"\n",
        "    # Mask irrelevant classes by setting them to 0\n",
        "    mask = tf.reduce_sum(tf.one_hot(irrelevant_classes, depth=num_classes), axis=0)\n",
        "    y_true_sample = y_true_sample * (1 - mask)\n",
        "\n",
        "    # Calculate class weights excluding irrelevant classes\n",
        "    class_weights = calculate_class_weights(y_true_sample, num_classes=num_classes, irrelevant_classes=irrelevant_classes)\n",
        "\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        # Apply the custom loss function with calculated weights\n",
        "        return custom_loss_function(y_true, y_pred, class_weights)\n",
        "\n",
        "    return loss_fn"
      ],
      "metadata": {
        "id": "E5crDo748lDz"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}